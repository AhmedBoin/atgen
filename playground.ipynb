{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Tensor Genetic Evolutionary Network\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The **Adaptive Tensor Genetic Evolutionary Network (ATGEN)** is an innovative approach in neural network design that combines traditional neural networks with genetic algorithms. The core idea is to dynamically evolve the structure and parameters of the neural network during training, thereby creating a highly adaptable model capable of learning complex patterns more efficiently.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "1. **Dynamic Network Evolution**: The network's structure is not fixed; it can evolve over time. Layers and neurons can be added or removed based on a set of evolutionary rules, which allows the network to adapt its complexity to the task at hand.\n",
    "\n",
    "2. **Genetic Algorithm-Based Training**: The network employs a genetic algorithm to evolve its weights and structure. This involves selecting parent networks based on their fitness, performing crossover to combine features of two parents, and applying mutations to introduce new variations.\n",
    "\n",
    "3. **Adaptive Activation Functions**: The network supports the dynamic adjustment of activation functions. This adaptability allows the model to switch from linear to non-linear transformations seamlessly, providing greater flexibility in learning different types of data distributions.\n",
    "\n",
    "4. **Neuron and Weight Management**: ATGEN includes mechanisms to add and remove neurons and weights in each layer. This helps in optimizing the network size and performance during the evolution process.\n",
    "\n",
    "## Implementation Overview\n",
    "\n",
    "### Components\n",
    "\n",
    "- **Linear Layer (`Linear` class)**: A custom linear layer with dynamic neuron management, including methods to add, remove, and initialize neurons and weights.\n",
    "- **Activation Switch (`ActiSwitch` class)**: A module that allows smooth transitions between different activation functions, controlled by a learnable parameter.\n",
    "- **Evolutionary Network (`ATNetwork` class)**: A neural network that evolves its structure and weights based on predefined rules and random mutations.\n",
    "- **Genetic Algorithm (`GeneticAlgorithm` class)**: A framework that orchestrates the evolutionary process, handling fitness evaluation, selection, crossover, mutation, and pruning of neurons.\n",
    "\n",
    "### Getting Started\n",
    "\n",
    "Below, we provide a step-by-step implementation of the Adaptive Tensor Genetic Evolution Network and demonstrate its unique capabilities through various experiments and tests. The notebook is divided into sections covering the core components, training routines, and network evolution strategies.\n",
    "\n",
    "---\n",
    "\n",
    "Let's dive into the implementation details and see how we can build a network that learns to evolve!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Import utility packages\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Import custom modules from the same folder\n",
    "from atgen.layers.activations import ActiSwitch, Pass\n",
    "from atgen.layers.linear import Linear\n",
    "from atgen.layers.conv import Conv2D\n",
    "from atgen.layers.maxpool import MaxPool2D\n",
    "from atgen.network import ATNetwork\n",
    "from atgen.memory import ReplayBuffer\n",
    "from atgen.utils import activation_functions\n",
    "from atgen.ga import ATGEN\n",
    "\n",
    "# Additional visualization or utility packages, if needed\n",
    "# from torchsummary import summary\n",
    "# import gymnasium as gym\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `Layer` Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. `ActiSwitch` with ReLU Activation\n",
    "\n",
    "The `ActiSwitch` class is tested with the `nn.Tanh()` activation function:\n",
    "\n",
    "- **Input**: A tensor of shape `(5, 4, 3, 2)` with random values.\n",
    "- **Output**: The resulting tensor after applying the activation function.\n",
    "- **Assertions**:\n",
    "  - The shape of the output tensor should match the shape of the input tensor.\n",
    "  - Check if the input and output tensors have the same values (i.e., the output tensor should be equal to the input tensor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: torch.Size([5, 4, 3, 2])\n",
      "Output: torch.Size([5, 4, 3, 2])\n",
      "No change in data: True\n"
     ]
    }
   ],
   "source": [
    "# 1. Testing ActiSwitch with ReLU Activation\n",
    "linear_pass_relu = ActiSwitch(nn.Tanh())\n",
    "x = torch.randn(5, 4, 3, 2)\n",
    "output: torch.Tensor = linear_pass_relu(x)\n",
    "\n",
    "print(f\"Input: {x.shape}\")\n",
    "print(f\"Output: {output.shape}\")\n",
    "print(f\"No change in data: {(x==output).all().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. `Linear` Layer Initialization\n",
    "\n",
    "The `Linear` layer is tested for its weight and bias initialization:\n",
    "\n",
    "- **Initial State**:\n",
    "  - Display the initial weights and biases of the `Linear` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights:\n",
      "Parameter containing:\n",
      "tensor([[-0.5440,  0.1003, -0.2166],\n",
      "        [-0.4120, -0.2642, -0.3355]], requires_grad=True)\n",
      "Initial bias:\n",
      "Parameter containing:\n",
      "tensor([-0.0079,  0.3244], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 2. Testing Linear Layer Initialization\n",
    "linear = Linear(in_features=3, out_features=2)\n",
    "\n",
    "print(\"Initial weights:\")\n",
    "print(linear.weight)\n",
    "print(\"Initial bias:\")\n",
    "print(linear.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Adding a Neuron to the `Linear` Layer\n",
    "\n",
    "Test the addition of a new output neuron:\n",
    "\n",
    "- **Action**: Add a new output neuron to the `Linear` layer.\n",
    "- **Assertions**:\n",
    "  - Display the updated weights and biases to verify the addition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After adding a new output neuron:\n",
      "Parameter containing:\n",
      "tensor([[-0.5440,  0.1003, -0.2166],\n",
      "        [-0.4120, -0.2642, -0.3355],\n",
      "        [-0.0289,  0.2317,  0.4608]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0079,  0.3244,  0.2280], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 3. Adding a Neuron\n",
    "linear.add_neuron()\n",
    "print(\"After adding a new output neuron:\")\n",
    "print(linear.weight)\n",
    "print(linear.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Increasing the Input Dimension of the `Linear` Layer\n",
    "\n",
    "Test the increase of the input dimension:\n",
    "\n",
    "- **Action**: Increase the input dimension of the `Linear` layer.\n",
    "- **Assertions**:\n",
    "  - Display the updated weights and biases to verify the change in input dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After increasing the input dimension:\n",
      "Parameter containing:\n",
      "tensor([[-0.5440,  0.1003, -0.2166,  0.0000],\n",
      "        [-0.4120, -0.2642, -0.3355,  0.0000],\n",
      "        [-0.0289,  0.2317,  0.4608,  0.0000]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0079,  0.3244,  0.2280], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 4. Increasing Input Dimension\n",
    "linear.add_weight()\n",
    "print(\"After increasing the input dimension:\")\n",
    "print(linear.weight)\n",
    "print(linear.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Removing a Neuron from the `Linear` Layer\n",
    "\n",
    "Test the removal of an output neuron:\n",
    "\n",
    "- **Action**: Remove an output neuron from the `Linear` layer.\n",
    "- **Assertions**:\n",
    "  - Display the updated weights and biases to verify the removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing an output neuron:\n",
      "Parameter containing:\n",
      "tensor([[-0.4120, -0.2642, -0.3355,  0.0000],\n",
      "        [-0.0289,  0.2317,  0.4608,  0.0000]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.3244, 0.2280], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 5. Removing a Neuron\n",
    "linear.remove_neuron(0)  # Assumes such a method exists\n",
    "print(\"After removing an output neuron:\")\n",
    "print(linear.weight)\n",
    "print(linear.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Removing an Input Weight from the `Linear` Layer\n",
    "\n",
    "Test the removal of an input weight:\n",
    "\n",
    "- **Action**: Remove an input weight from the `Linear` layer.\n",
    "- **Assertions**:\n",
    "  - Display the updated weights and biases to verify the removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing an input weight:\n",
      "Parameter containing:\n",
      "tensor([[-0.2642, -0.3355,  0.0000],\n",
      "        [ 0.2317,  0.4608,  0.0000]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.3244, 0.2280], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 6. Removing an Input Weight\n",
    "linear.remove_weight(0)  # Assumes such a method exists\n",
    "print(\"After removing an input weight:\")\n",
    "print(linear.weight)\n",
    "print(linear.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Identity Initialization of the `Linear` Layer\n",
    "\n",
    "Test the identity initialization of the `Linear` layer:\n",
    "\n",
    "- **Action**: Initialize a `Linear` layer with identity weights.\n",
    "- **Assertions**:\n",
    "  - Display the weights and biases to verify the identity initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights after identity initialization:\n",
      "Parameter containing:\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]], requires_grad=True)\n",
      "Bias after identity initialization:\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 7. Testing Identity Initialization\n",
    "identity_layer = Linear.init_identity_layer(size=3)\n",
    "print(\"Weights after identity initialization:\")\n",
    "print(identity_layer.weight)\n",
    "print(\"Bias after identity initialization:\")\n",
    "print(identity_layer.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Identity Matrix in Neural Networks\n",
    "\n",
    "### Identity Matrix\n",
    "\n",
    "In the context of neural networks, the identity matrix is often used as an initialization method for weights in newly added layers. The identity matrix is a square matrix with ones on the main diagonal and zeros elsewhere. It has the property that when used in matrix multiplication, it does not alter the original matrix:\n",
    "\n",
    "$$\n",
    "I_n = \\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 1 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where $I_n$ is the identity matrix of size $n \\times n$. This matrix is called the identity matrix because it acts as a multiplicative identity in matrix multiplication.\n",
    "\n",
    "### Matrix Multiplication with the Identity Matrix\n",
    "\n",
    "When you multiply a matrix $A$ by an identity matrix $I$, the result is the matrix $A$ itself:\n",
    "\n",
    "$$\n",
    "A \\times I = A\n",
    "$$\n",
    "\n",
    "For example, consider a matrix $A$:\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "a & b \\\\\n",
    "c & d\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Multiplying $A$ by the $2 \\times 2$ identity matrix $I$:\n",
    "\n",
    "$$\n",
    "A \\times I_2 = \\begin{bmatrix}\n",
    "a & b \\\\\n",
    "c & d\n",
    "\\end{bmatrix}\n",
    "\\times\n",
    "\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a & b \\\\\n",
    "c & d\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The result is $A$, demonstrating that the identity matrix does not change the matrix it multiplies.\n",
    "\n",
    "### Usage in Neural Networks\n",
    "\n",
    "In neural networks, initializing weights with an identity matrix ensures that newly added layers do not alter the input significantly when first added. This can be particularly useful when experimenting with network architectures or adding new neurons, as it maintains stability in the output until training adjusts the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Activation Functions in Deep Networks\n",
    "\n",
    "As neural networks become deeper, the introduction of new layers can affect the output of the network due to the activation functions applied. Even if the new layers are initialized with identity matrices, which theoretically do not alter the output directly, activation functions introduce non-linearity that can change the network's behavior.\n",
    "\n",
    "### The Problem with Activation Functions\n",
    "\n",
    "When adding new layers with identity matrix initialization, the direct output of these layers may remain unchanged. However, if the activation functions applied to these layers are non-linear, they can alter the output in ways that affect the network's overall performance. This introduces a challenge: ensuring that the output remains consistent while still benefiting from the added layers.\n",
    "\n",
    "### Solution: Smooth Transition with Blending Activation Functions\n",
    "\n",
    "To address this issue, a custom approach is used to blend linear and non-linear transformations. The idea is to smoothly transition between a linear pass-through and a non-linear activation function, based on a learnable weighting factor. This ensures that new layers can be added without introducing abrupt changes in output.\n",
    "\n",
    "One effective solution is to use a class that blends linear and non-linear activations. This class provides a learnable parameter that adjusts the balance between the linear transformation and the non-linear activation function. By controlling this parameter, the network can maintain the output consistency while allowing the flexibility to benefit from the additional layers.\n",
    "\n",
    "### Key Points\n",
    "\n",
    "- **Identity Matrix**: Ensures that additional layers do not affect the output directly but does not account for changes introduced by activation functions.\n",
    "- **Linear and Semi-Linear Activation Functions**: Maintain output consistency while adding layers, avoiding undesirable performance changes.\n",
    "- **Blending Mechanism**: Allows a smooth transition between linear and non-linear functions, controlled by a learnable parameter, to ensure the network adapts effectively.\n",
    "\n",
    "This approach helps in maintaining the performance and output consistency of deep networks while accommodating new layers and activation functions.\n",
    "```​⬤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[38;5;153mModel Summary\u001b[0m\u001b[1m:\n",
      "-------------------------------------------------------------------------------------\n",
      "Layer      Output Shape                  Parameters     Activation     \n",
      "-------------------------------------------------------------------------------------\n",
      "Layer 1    (batch_size, 3)               18             ActiSwitch(ReLU, 0.00%)\n",
      "Layer 2    (batch_size, 1)               4              Pass           \n",
      "-------------------------------------------------------------------------------------\n",
      "\u001b[38;5;153mTotal Parameters:        \u001b[0m\u001b[1m22             \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Helper loss function\n",
    "def loss(x1: torch.Tensor, x2: torch.Tensor):\n",
    "    val = torch.abs((x2 / x1).mean() - 1) * 100\n",
    "    print(f\"loss = {val:.10f}%\")\n",
    "\n",
    "# Define the Custom Network\n",
    "class CustomNetwork(ATNetwork):\n",
    "    def __init__(self):\n",
    "        # Do not call the parent __init__ with layers; initialize manually\n",
    "        super(ATNetwork, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            Linear(5, 3),\n",
    "            Linear(3, 1)\n",
    "        ])\n",
    "        self.activation = nn.ModuleList([\n",
    "            ActiSwitch(),\n",
    "            Pass()\n",
    "        ])\n",
    "        self.backprob_phase = True\n",
    "        self.default_activation = nn.ReLU()\n",
    "\n",
    "# Initialize the model and print the summary\n",
    "model = CustomNetwork()\n",
    "# summary(model, input_size=(5,))\n",
    "model.summary()\n",
    "\n",
    "# You can also initialize the model simply like next\n",
    "model = ATNetwork([5, 3, 1])\n",
    "\n",
    "# Test the network with random input\n",
    "x = torch.randn(4, 5)\n",
    "y1: torch.Tensor = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[38;5;153mModel Summary\u001b[0m\u001b[1m:\n",
      "-------------------------------------------------------------------------------------\n",
      "Layer      Output Shape                  Parameters     Activation     \n",
      "-------------------------------------------------------------------------------------\n",
      "Layer 1    (batch_size, 5)               30             ActiSwitch(ReLU, 0.00%)\n",
      "Layer 2    (batch_size, 5)               30             ActiSwitch(ReLU, 0.00%)\n",
      "Layer 3    (batch_size, 5)               30             ActiSwitch(ReLU, 0.00%)\n",
      "Layer 4    (batch_size, 5)               30             ActiSwitch(ReLU, 0.00%)\n",
      "Layer 5    (batch_size, 5)               30             ActiSwitch(ReLU, 0.00%)\n",
      "Layer 6    (batch_size, 5)               30             ActiSwitch(ReLU, 0.00%)\n",
      "Layer 7    (batch_size, 5)               30             ActiSwitch(ReLU, 0.00%)\n",
      "Layer 8    (batch_size, 5)               30             ActiSwitch(ReLU, 0.00%)\n",
      "Layer 9    (batch_size, 3)               18             ActiSwitch(ReLU, 0.00%)\n",
      "Layer 10   (batch_size, 3)               12             ActiSwitch(ReLU, 0.00%)\n",
      "Layer 11   (batch_size, 3)               12             ActiSwitch(ReLU, 0.00%)\n",
      "Layer 12   (batch_size, 1)               4              Pass           \n",
      "-------------------------------------------------------------------------------------\n",
      "\u001b[38;5;153mTotal Parameters:        \u001b[0m\u001b[1m286            \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Evolve the network by inserting new 10 layers\n",
    "for _ in range(10):\n",
    "    model.evolve_network()\n",
    "# summary(model, input_size=(5,))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[38;5;153mModel Summary\u001b[0m\u001b[1m:\n",
      "-------------------------------------------------------------------------------------\n",
      "Layer      Output Shape                  Parameters     Activation     \n",
      "-------------------------------------------------------------------------------------\n",
      "Layer 1    (batch_size, 54)              324            ActiSwitch(ReLU, 0.00%)\n",
      "Layer 2    (batch_size, 48)              2640           ActiSwitch(ReLU, 0.00%)\n",
      "Layer 3    (batch_size, 54)              2646           ActiSwitch(ReLU, 0.00%)\n",
      "Layer 4    (batch_size, 45)              2475           ActiSwitch(ReLU, 0.00%)\n",
      "Layer 5    (batch_size, 53)              2438           ActiSwitch(ReLU, 0.00%)\n",
      "Layer 6    (batch_size, 54)              2916           ActiSwitch(ReLU, 0.00%)\n",
      "Layer 7    (batch_size, 41)              2255           ActiSwitch(ReLU, 0.00%)\n",
      "Layer 8    (batch_size, 54)              2268           ActiSwitch(ReLU, 0.00%)\n",
      "Layer 9    (batch_size, 50)              2750           ActiSwitch(ReLU, 0.00%)\n",
      "Layer 10   (batch_size, 46)              2346           ActiSwitch(ReLU, 0.00%)\n",
      "Layer 11   (batch_size, 50)              2350           ActiSwitch(ReLU, 0.00%)\n",
      "Layer 12   (batch_size, 1)               51             Pass           \n",
      "-------------------------------------------------------------------------------------\n",
      "\u001b[38;5;153mTotal Parameters:        \u001b[0m\u001b[1m25459          \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Evolve layers by adding new 500 nodes to random layers\n",
    "for _ in range(500):\n",
    "    model.evolve_layer()\n",
    "# summary(model, input_size=(5,))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0970, -1.0970],\n",
      "        [-0.3942, -0.3942],\n",
      "        [-1.5667, -1.5667],\n",
      "        [-0.4394, -0.4394]], grad_fn=<CatBackward0>)\n",
      "loss = 0.0000059605%\n"
     ]
    }
   ],
   "source": [
    "# Print the concatenated results and loss\n",
    "y2: torch.Tensor = model(x)\n",
    "print(torch.cat((y1, y2), dim=1))  \n",
    "loss(y1, y2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[38;5;153mModel Summary\u001b[0m\u001b[1m:\n",
      "-------------------------------------------------------------------------------------\n",
      "Layer      Output Shape                  Parameters     Activation     \n",
      "-------------------------------------------------------------------------------------\n",
      "Layer 1    (batch_size, 54)              324            ActiSwitch(ReLU, 0.00%)\n",
      "Layer 2    (batch_size, 48)              2640           ActiSwitch(ReLU, 0.00%)\n",
      "Layer 3    (batch_size, 54)              2646           ActiSwitch(ReLU, 0.00%)\n",
      "Layer 4    (batch_size, 45)              2475           ActiSwitch(ReLU, 0.00%)\n",
      "Layer 5    (batch_size, 53)              2438           ActiSwitch(ReLU, 0.00%)\n",
      "Layer 6    (batch_size, 54)              2916           ActiSwitch(ReLU, 0.00%)\n",
      "Layer 7    (batch_size, 41)              2255           ActiSwitch(ReLU, 0.00%)\n",
      "Layer 8    (batch_size, 54)              2268           ActiSwitch(ReLU, 0.00%)\n",
      "Layer 9    (batch_size, 50)              2750           ActiSwitch(ReLU, 0.00%)\n",
      "Layer 10   (batch_size, 46)              2346           ActiSwitch(ReLU, 0.00%)\n",
      "Layer 11   (batch_size, 50)              2350           ActiSwitch(ReLU, 0.00%)\n",
      "Layer 12   (batch_size, 1)               51             Pass           \n",
      "-------------------------------------------------------------------------------------\n",
      "\u001b[38;5;153mTotal Parameters:        \u001b[0m\u001b[1m25459          \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Get the output after evolution and pruning\n",
    "with torch.no_grad():\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, Linear) or str(type(layer)) == \"<class 'layers.Linear'>\":\n",
    "            layer.weight.data *= 0.03\n",
    "            if layer.bias is not None:\n",
    "                layer.bias.data *= 0.03\n",
    "    # for layer in model.activation:\n",
    "    #     if isinstance(layer, ActiSwitch) or str(type(layer)) == \"<class 'layers.ActiSwitch'>\":\n",
    "    #         layer.weight.data.copy_(torch.tensor(1.0))\n",
    "\n",
    "model.prune(0.01)\n",
    "# summary(model, input_size=(5,))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Crossover works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[38;5;153mModel Summary\u001b[0m\u001b[1m:\n",
      "-------------------------------------------------------------------------------------\n",
      "Layer      Output Shape                  Parameters     Activation     \n",
      "-------------------------------------------------------------------------------------\n",
      "Layer 1    (batch_size, 54)              324            ActiSwitch(ReLU, 0.00%)\n",
      "Layer 2    (batch_size, 48)              2640           ActiSwitch(ReLU, 0.00%)\n",
      "Layer 3    (batch_size, 54)              2646           ActiSwitch(ReLU, 0.00%)\n",
      "Layer 4    (batch_size, 45)              2475           ActiSwitch(ReLU, 0.00%)\n",
      "Layer 5    (batch_size, 53)              2438           ActiSwitch(ReLU, 0.00%)\n",
      "Layer 6    (batch_size, 54)              2916           ActiSwitch(ReLU, 0.00%)\n",
      "Layer 7    (batch_size, 41)              2255           ActiSwitch(ReLU, 0.00%)\n",
      "Layer 8    (batch_size, 54)              2268           ActiSwitch(ReLU, 0.00%)\n",
      "Layer 9    (batch_size, 50)              2750           ActiSwitch(ReLU, 0.00%)\n",
      "Layer 10   (batch_size, 46)              2346           ActiSwitch(ReLU, 0.00%)\n",
      "Layer 11   (batch_size, 50)              2350           ActiSwitch(ReLU, 0.00%)\n",
      "Layer 12   (batch_size, 1)               51             Pass           \n",
      "-------------------------------------------------------------------------------------\n",
      "\u001b[38;5;153mTotal Parameters:        \u001b[0m\u001b[1m25459          \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Create parent networks with specified architectures\n",
    "parent1 = ATNetwork([10, 2, 5, 1])\n",
    "parent2 = ATNetwork([10, 2, 4, 3, 1])\n",
    "\n",
    "# Initialize the GeneticAlgorithm instance\n",
    "ga = ATGEN(population_size=10, layers=[8, 1, 4])\n",
    "\n",
    "# Perform crossover\n",
    "child = ga.crossover(parent1, parent2)\n",
    "\n",
    "# Print the child network architecture and parameters\n",
    "# summary(child, (10,))\n",
    "model.summary()\n",
    "\n",
    "# ga.run_generation(dummy_fitness_function, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[38;5;153mModel Summary\u001b[0m\u001b[1m:\n",
      "-------------------------------------------------------------------------------------\n",
      "Layer      Output Shape                  Parameters     Activation     \n",
      "-------------------------------------------------------------------------------------\n",
      "Layer 1    (batch_size, 4)               8              ActiSwitch(ReLU, 0.00%)\n",
      "Layer 2    (batch_size, 1)               5              Pass           \n",
      "-------------------------------------------------------------------------------------\n",
      "\u001b[38;5;153mTotal Parameters:        \u001b[0m\u001b[1m13             \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = ATNetwork([1, 4, 1])\n",
    "optim = AdamW(model.parameters(), lr=0.01)\n",
    "# summary(model, (1,))\n",
    "model.summary()\n",
    "\n",
    "x = torch.randn(10, 1)\n",
    "y_real = torch.randn(10, 1)\n",
    "epochs = 1_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.08334087580442429\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    y_pred = model(x)\n",
    "    loss = F.mse_loss(y_pred, y_real)\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "print(\"Loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.07375232130289078\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    y_pred = model(x)\n",
    "    loss = F.mse_loss(y_pred, y_real)\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "print(\"Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[38;5;153mModel Summary\u001b[0m\u001b[1m:\n",
      "-------------------------------------------------------------------------------------\n",
      "Layer      Output Shape                  Parameters     Activation     \n",
      "-------------------------------------------------------------------------------------\n",
      "Layer 1    (batch_size, 13)              26             ActiSwitch(ReLU, 0.00%)\n",
      "Layer 2    (batch_size, 25)              350            ActiSwitch(ReLU, 0.00%)\n",
      "Layer 3    (batch_size, 23)              598            ActiSwitch(ReLU, 263.90%)\n",
      "Layer 4    (batch_size, 21)              504            ActiSwitch(ReLU, 0.00%)\n",
      "Layer 5    (batch_size, 12)              264            ActiSwitch(ReLU, 0.00%)\n",
      "Layer 6    (batch_size, 24)              312            ActiSwitch(ReLU, 0.00%)\n",
      "Layer 7    (batch_size, 1)               25             Pass           \n",
      "-------------------------------------------------------------------------------------\n",
      "\u001b[38;5;153mTotal Parameters:        \u001b[0m\u001b[1m2079           \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    model.evolve_network()\n",
    "for _ in range(100):\n",
    "    model.evolve_layer()\n",
    "optim = AdamW(model.parameters(), lr=0.01)\n",
    "# summary(model, (1,))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0797,  0.2908,  0.1687, -0.1020,  0.1894,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1081,  0.2916, -0.0104,  0.3621, -0.0848,  0.2051,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2869, -0.2862,  0.2688,  0.0379,  0.0088,  0.3320,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1010, -0.3674,  0.2282,  0.3868,  0.1258,  0.1762,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3398,  0.0891, -0.0769,  0.1613, -0.3579, -0.0521,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0408,  0.0056,  0.0667, -0.0160, -0.3470, -0.1292,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2645, -0.0831, -0.2908,  0.1478,  0.2869,  0.1338, -0.3515,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2861,  0.0818,  0.2982, -0.2815,  0.0611, -0.0096,  0.1076,  0.1832,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1406, -0.0800, -0.3013, -0.1177,  0.1759, -0.2215, -0.1695,  0.1930,\n",
      "         -0.0584,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2159,  0.0239, -0.1732, -0.0787,  0.1450,  0.3194, -0.1228, -0.1541,\n",
      "         -0.0503,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1660,  0.0890,  0.1922, -0.1157, -0.2649,  0.0963, -0.1625,  0.1679,\n",
      "          0.0457,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1713, -0.1855,  0.2932, -0.3030,  0.1740,  0.0132, -0.1782,  0.1148,\n",
      "          0.2017,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1669, -0.2620,  0.0235,  0.1022, -0.3128, -0.0256,  0.1703,  0.0359,\n",
      "         -0.2646, -0.3065,  0.0000,  0.0000],\n",
      "        [-0.1785, -0.3122,  0.2109, -0.1045, -0.2937,  0.0871,  0.0858,  0.0989,\n",
      "         -0.0804,  0.0091,  0.0000,  0.0000],\n",
      "        [-0.2041,  0.1438,  0.2053,  0.2944, -0.1748,  0.1750, -0.0762, -0.1508,\n",
      "         -0.2691,  0.2297, -0.2765,  0.0000],\n",
      "        [ 0.0487,  0.2845,  0.1641,  0.1253,  0.2977, -0.0030,  0.0322,  0.0339,\n",
      "         -0.2421,  0.1668,  0.0185,  0.0000],\n",
      "        [ 0.0033,  0.0593, -0.2158, -0.2492,  0.2763, -0.0871,  0.0103,  0.2483,\n",
      "         -0.1127,  0.1394,  0.1467, -0.1248],\n",
      "        [ 0.2289, -0.2699,  0.1517,  0.1280,  0.2636, -0.0798, -0.2165, -0.1240,\n",
      "         -0.2103,  0.0807,  0.2392,  0.1926],\n",
      "        [-0.0840,  0.0238,  0.0349,  0.0993,  0.0599,  0.0282, -0.2554, -0.2167,\n",
      "         -0.1162, -0.2683,  0.2592, -0.1360],\n",
      "        [ 0.2393,  0.1102,  0.1867,  0.1229, -0.1336, -0.2758, -0.0735,  0.2123,\n",
      "          0.2368, -0.1663, -0.2558, -0.1486]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[5].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.06089594215154648\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    y_pred = model(x)\n",
    "    loss = F.mse_loss(y_pred, y_real)\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "print(\"Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 9.4987e-01,  3.1045e-02,  1.3883e-02, -4.3404e-02, -7.1988e-03,\n",
      "          2.7078e-03, -1.1177e-03, -4.7752e-03, -3.1626e-03, -2.7261e-03,\n",
      "          1.9394e-02,  6.3349e-02],\n",
      "        [ 4.6074e-02,  9.3652e-01,  1.4739e-02, -4.4814e-02, -9.8379e-03,\n",
      "          2.3553e-03, -3.7962e-04, -5.8734e-03, -3.4815e-03, -3.1531e-03,\n",
      "          2.0521e-02,  6.6690e-02],\n",
      "        [ 4.1279e-02,  2.9368e-02,  9.1569e-01, -4.4808e-02, -6.6768e-03,\n",
      "          1.9588e-03, -6.6284e-05, -5.5333e-03, -2.6283e-03, -2.5344e-03,\n",
      "          1.9336e-02,  6.1580e-02],\n",
      "        [-4.9911e-02, -2.8189e-02, -1.8601e-02,  9.4594e-01,  8.6403e-03,\n",
      "         -4.2184e-03,  4.1439e-03,  2.1420e-03,  4.0350e-03,  2.1600e-03,\n",
      "         -2.7487e-02, -7.0973e-02],\n",
      "        [ 1.4019e-03,  1.0511e-01,  2.0395e-01, -1.4025e-01,  3.2255e-01,\n",
      "          4.6527e-02, -5.6269e-02,  3.3189e-02, -2.1864e-02,  3.6151e-02,\n",
      "         -1.1192e-02, -1.0336e-01],\n",
      "        [ 3.5546e-02,  1.3929e-01, -7.9383e-02,  2.0725e-01, -4.3356e-02,\n",
      "          1.4975e-01,  3.4361e-02, -4.5289e-02,  1.7928e-03,  1.5825e-02,\n",
      "         -1.1123e-02, -1.4782e-02],\n",
      "        [ 1.0763e-01, -2.0160e-01,  1.2173e-01,  6.4029e-02, -7.4259e-02,\n",
      "          2.0792e-01,  8.7732e-02, -4.2561e-02,  4.1500e-02,  1.5751e-02,\n",
      "          6.5890e-02, -6.2897e-03],\n",
      "        [-4.8079e-02, -2.7423e-01,  9.1809e-02,  2.8367e-01, -2.0723e-03,\n",
      "          4.3614e-02,  9.2530e-02, -1.0163e-01,  4.4371e-02, -4.9039e-02,\n",
      "          1.4384e-01,  1.1925e-01],\n",
      "        [-3.8413e-01, -6.1536e-02, -1.2692e-01,  5.5605e-02, -2.3113e-01,\n",
      "          4.3651e-02, -4.1364e-02, -3.0139e-03, -3.1648e-03,  2.0662e-03,\n",
      "         -1.9733e-01, -2.0216e-01],\n",
      "        [ 3.0528e-02,  3.6131e-03,  3.2306e-02, -4.6703e-02, -2.3712e-01,\n",
      "         -7.3802e-02,  3.4927e-02,  7.5392e-03,  4.9861e-03,  6.3925e-02,\n",
      "         -1.2850e-01, -1.3399e-01],\n",
      "        [ 1.5174e-01, -4.2020e-02, -3.5365e-01, -2.2340e-03,  1.8615e-01,\n",
      "          7.2011e-02, -2.4574e-01, -8.1613e-02,  5.3448e-02, -4.1859e-02,\n",
      "          3.5211e-02,  8.6516e-02],\n",
      "        [ 2.1393e-01,  7.8478e-02,  2.2235e-01, -4.1379e-01, -2.7150e-02,\n",
      "         -1.3296e-01,  1.6215e-01,  7.1936e-02, -4.1162e-02, -1.3095e-01,\n",
      "          4.4470e-01,  4.3517e-01],\n",
      "        [-1.3740e-01, -3.4407e-02, -2.5407e-01, -2.3731e-02,  1.5454e-01,\n",
      "         -1.2722e-01, -2.0177e-01,  2.3478e-01, -9.5601e-02,  2.5624e-02,\n",
      "         -1.6587e-01, -1.4889e-01],\n",
      "        [ 1.4876e-01, -9.4304e-03, -2.1086e-01, -1.1050e-01,  1.4328e-01,\n",
      "          3.0857e-01, -7.8004e-02, -1.6440e-01,  7.2154e-03, -2.2776e-02,\n",
      "          1.2664e-02, -2.1477e-02],\n",
      "        [ 5.7338e-02,  1.4287e-02,  7.9318e-02, -1.6248e-01, -2.0330e-01,\n",
      "          4.9324e-02, -9.3014e-02,  1.1455e-01,  8.4867e-02, -1.5566e-03,\n",
      "          4.2206e-02, -4.7375e-02],\n",
      "        [ 7.4948e-02, -1.0776e-01,  2.1472e-01, -1.7331e-01,  1.0501e-01,\n",
      "         -5.5413e-02, -1.2042e-01,  1.1393e-01,  1.9766e-01,  7.9320e-04,\n",
      "          7.9561e-02,  4.8266e-03],\n",
      "        [-1.0891e-01, -1.4355e-01,  7.4580e-02,  1.7492e-01, -3.1579e-01,\n",
      "         -3.9965e-02,  1.3435e-01,  6.5024e-02, -2.9158e-01, -2.2713e-01,\n",
      "         -2.0978e-03,  1.9809e-02],\n",
      "        [-2.3178e-01, -2.3009e-01,  1.6543e-01,  7.0650e-02, -3.6687e-01,\n",
      "          6.5124e-02,  1.0371e-01,  1.0773e-01, -2.3507e-02, -6.4940e-02,\n",
      "          2.8172e-02, -2.0593e-02],\n",
      "        [-3.2919e-01,  8.7950e-04,  7.6171e-02,  1.7513e-01, -1.4220e-01,\n",
      "          1.7299e-01, -3.6979e-02, -1.8162e-01, -2.1347e-01,  1.9081e-01,\n",
      "         -2.8718e-01, -5.7156e-02],\n",
      "        [ 1.9921e-02,  1.5644e-01,  1.1327e-01,  3.5994e-02,  3.0021e-01,\n",
      "         -7.4442e-03,  4.2160e-02,  5.5108e-03, -2.1156e-01,  1.6102e-01,\n",
      "         -1.9135e-02, -3.1862e-02],\n",
      "        [ 1.3365e-01,  1.1102e-01, -7.3777e-02, -1.4887e-01,  2.3000e-01,\n",
      "         -8.6411e-02, -4.4253e-02,  2.6159e-01, -1.1906e-01,  8.8626e-02,\n",
      "          2.9234e-01, -1.1607e-02],\n",
      "        [ 7.5326e-02, -9.8233e-02,  1.9082e-02,  1.0125e-01,  1.5209e-01,\n",
      "         -1.7628e-01, -1.0370e-01, -1.6340e-01, -1.6956e-01,  9.5955e-02,\n",
      "          2.7372e-01,  2.2467e-01],\n",
      "        [-5.4394e-02,  2.1072e-02,  3.6359e-02,  3.4638e-02,  6.4158e-02,\n",
      "          1.6012e-03, -2.2496e-01, -2.1460e-01, -1.4733e-01, -1.8123e-01,\n",
      "          1.7410e-01, -1.1805e-01],\n",
      "        [ 1.0697e-01,  2.2156e-02,  4.3647e-02, -1.4044e-01, -1.3164e-01,\n",
      "         -2.9638e-01,  1.1334e-02,  9.7535e-02,  2.7045e-01, -1.7785e-01,\n",
      "         -2.0704e-01, -8.7570e-02]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[5].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
