{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Don't Repeat Yourself!\n",
    "## Keep the Momentum: Evolving Neural Networks Without Starting Over\n",
    "\n",
    "Training deep neural networks is a time-intensive process, often requiring hours, days, or even weeks of fine-tuning on high-powered GPUs. However, when a network saturates and no longer improves in accuracy, conventional approaches would call for retraining from scratch with a new, deeper, or wider architecture. This can lead to wasted computational resources and long downtimes as models are redesigned and retrained from the beginning. Our innovation addresses this challenge by allowing a trained neural network to evolve without losing any of its achieved accuracy. With this approach, networks can continue learning and refining their performance without the need to start over, effectively speeding up the training process and boosting productivity.\n",
    "\n",
    "### Layer Insertion and Modification Without Accuracy Loss\n",
    "\n",
    "When a neural network hits its performance ceiling, it may require a more complex architecture—deeper layers, additional neurons, or more filters in convolutional layers—to push accuracy higher. Traditionally, this would mean designing a new model and restarting training from scratch. However, our method allows for the seamless insertion of new layers, neurons, or filters into a trained network, all without compromising the current accuracy level.\n",
    "\n",
    "The key lies in how weights are initialized. For fully connected layers, we insert identity matrices for the new layers, preserving the function of the existing network. In convolutional layers, we use identity filters, which act similarly to the identity matrix but for feature maps. This ensures that the newly inserted layers do not alter the input-output relationship learned by the network, maintaining the accuracy already achieved. Additionally, when neurons are added to existing layers, their weights are initialized using standard initialization techniques, but the connections to subsequent layers are zero-weighted. This setup guarantees that the new neurons do not interfere with the performance of the already trained part of the network.\n",
    "\n",
    "### Handling Non-Linearity with the ActiSwitch Layer\n",
    "\n",
    "Introducing new layers or neurons also brings the challenge of activation functions. Activation functions play a crucial role in introducing non-linearity, and their behavior can impact how well new components integrate into an existing network. To solve this, we have developed the ActiSwitch layer—a mechanism that allows a smooth transition between linear and non-linear activation functions.\n",
    "\n",
    "The ActiSwitch layer operates using two parameters that control the ratio between linearity and non-linearity, creating a dynamic blend between the two extremes. As the network trains, the model can adjust these parameters to smoothly switch between linear behavior and the desired non-linearity. This capability is particularly valuable when adding new neurons or layers, as it allows the network to incorporate the new elements without destabilizing the already trained sections. The ActiSwitch layer ensures that activation functions evolve in sync with the expanded architecture, providing a smooth learning curve for the newly added components.\n",
    "\n",
    "### Increasing Productivity in Neural Network Research\n",
    "\n",
    "In the fast-paced world of AI research, productivity is paramount. This method accelerates the iterative process of neural network design, enabling faster experimentation without the need to restart each time a change is made to the architecture. Instead of retraining from scratch, researchers can continue from where they left off, modifying the network incrementally to achieve better performance.\n",
    "\n",
    "Furthermore, this technique is highly adaptable. Whether you need to insert a few extra neurons in a fully connected layer, expand the number of filters in a convolutional layer, or even alter the size of filters themselves, our method can accommodate these changes without disrupting the training process. It provides an efficient, flexible way to adapt and scale neural networks without sacrificing prior progress.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "from atgen.network import ATNetwork\n",
    "from atgen.layers import Linear, Flatten, Conv2D, MaxPool2D, ActiSwitch, Pass\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 128\n",
    "\n",
    "# CIFAR-100 dataset (100 classes, 32x32 images)\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR100(root='../data', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.CIFAR100(root='../data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[38;5;153mModel Summary\u001b[0m\u001b[1m:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Layer      Type           Output Shape                  Parameters     Activation     \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Layer 1    Conv2D         (batch_size, 32, 32, 32)      896            ActiSwitch(ReLU, 100.00%)\n",
      "Layer 2    MaxPool2D      (batch_size, 32, 16, 16)      0              Pass           \n",
      "Layer 3    Conv2D         (batch_size, 64, 16, 16)      18496          ActiSwitch(ReLU, 100.00%)\n",
      "Layer 4    MaxPool2D      (batch_size, 64, 8, 8)        0              Pass           \n",
      "Layer 5    Flatten        (batch_size, 4096)            0              Pass           \n",
      "Layer 6    Linear         (batch_size, 100)             409700         Pass           \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\u001b[38;5;153mTotal Parameters:        \u001b[0m\u001b[1m429,092\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = ATNetwork(\n",
    "    Conv2D(3, 32, kernel_size=3, norm=True),\n",
    "    ActiSwitch(nn.ReLU),\n",
    "    MaxPool2D(),\n",
    "    Conv2D(32, 64, kernel_size=3, norm=True),\n",
    "    ActiSwitch(nn.ReLU),\n",
    "    MaxPool2D(),\n",
    "    Flatten(),\n",
    "    Linear(64*8*8, 100),\n",
    "    input_size=(32, 32)\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize network and optimizer\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(model, train_loader, num_epochs):\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Track accuracy\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        accuracy = 100. * correct / total\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/i:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "    print(f'Training completed in: {training_time:.2f} seconds')\n",
    "    return training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate function\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    accuracy = 100. * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the initial network...\n",
      "Epoch [1/10], Loss: 3.6536, Accuracy: 16.84%\n",
      "Epoch [2/10], Loss: 3.0037, Accuracy: 27.24%\n",
      "Epoch [3/10], Loss: 2.7334, Accuracy: 32.54%\n",
      "Epoch [4/10], Loss: 2.5920, Accuracy: 35.64%\n",
      "Epoch [5/10], Loss: 2.4756, Accuracy: 37.80%\n",
      "Epoch [6/10], Loss: 2.3892, Accuracy: 39.63%\n",
      "Epoch [7/10], Loss: 2.3144, Accuracy: 41.38%\n",
      "Epoch [8/10], Loss: 2.2570, Accuracy: 42.30%\n",
      "Epoch [9/10], Loss: 2.2051, Accuracy: 43.81%\n",
      "Epoch [10/10], Loss: 2.1673, Accuracy: 44.64%\n",
      "Training completed in: 261.27 seconds\n",
      "Test Accuracy: 41.09%\n"
     ]
    }
   ],
   "source": [
    "# Training the initial network\n",
    "print(\"Training the initial network...\")\n",
    "initial_training_time = train_model(model, train_loader, num_epochs)\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now you can apply the method we’ve discussed (inserting new layers, neurons, or filters) to enhance performance without resetting the training process.\n",
    "You’ll also notice that the accuracy starts off significantly higher than with the traditional CNN. This improvement is thanks to the `ActiSwitch` layer, which strikes an optimal balance between linearity and non-linearity. As we explored earlier, `ActiSwitch` not only outperforms traditional skip connections but also shows great promise as an architecture that can surpass the capabilities of `ResNet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[38;5;153mModel Summary\u001b[0m\u001b[1m:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Layer      Type           Output Shape                  Parameters     Activation     \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Layer 1    Conv2D         (batch_size, 32, 32, 32)      896            ActiSwitch(ReLU, 78.79 %)\n",
      "Layer 2    Conv2D         (batch_size, 32, 32, 32)      9248           ActiSwitch(ReLU, 0.00  %)\n",
      "Layer 3    MaxPool2D      (batch_size, 32, 16, 16)      0              Pass           \n",
      "Layer 4    Conv2D         (batch_size, 64, 16, 16)      18496          ActiSwitch(ReLU, 97.36 %)\n",
      "Layer 5    Conv2D         (batch_size, 64, 16, 16)      36928          ActiSwitch(ReLU, 0.00  %)\n",
      "Layer 6    MaxPool2D      (batch_size, 64, 8, 8)        0              Pass           \n",
      "Layer 7    Flatten        (batch_size, 4096)            0              Pass           \n",
      "Layer 8    Linear         (batch_size, 100)             409700         ActiSwitch(ReLU, 0.00  %)\n",
      "Layer 9    Linear         (batch_size, 100)             10100          Pass           \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\u001b[38;5;153mTotal Parameters:        \u001b[0m\u001b[1m485,368\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model.layers.insert(1, Conv2D.init_identity_layer(32, kernel_size=3, norm=True))\n",
    "model.layers.insert(4, Conv2D.init_identity_layer(64, kernel_size=3, norm=True))\n",
    "model.layers.insert(8, Linear.init_identity_layer(100))\n",
    "model.activation.insert(1, ActiSwitch(nn.ReLU, True))\n",
    "model.activation.insert(4, ActiSwitch(nn.ReLU, True))\n",
    "model.activation.insert(7, ActiSwitch(nn.ReLU, True))\n",
    "model.store_sizes((32, 32))\n",
    "model.summary()\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continue training the network...\n",
      "Epoch [1/10], Loss: 2.9718, Accuracy: 40.72%\n",
      "Epoch [2/10], Loss: 2.0917, Accuracy: 46.24%\n",
      "Epoch [3/10], Loss: 2.0208, Accuracy: 47.49%\n",
      "Epoch [4/10], Loss: 1.9759, Accuracy: 48.70%\n",
      "Epoch [5/10], Loss: 1.9405, Accuracy: 49.46%\n",
      "Epoch [6/10], Loss: 1.8927, Accuracy: 50.40%\n",
      "Epoch [7/10], Loss: 1.8541, Accuracy: 51.28%\n",
      "Epoch [8/10], Loss: 1.8206, Accuracy: 51.68%\n",
      "Epoch [9/10], Loss: 1.7869, Accuracy: 52.70%\n",
      "Epoch [10/10], Loss: 1.7584, Accuracy: 53.31%\n",
      "Training completed in: 446.89 seconds\n",
      "Test Accuracy: 46.42%\n"
     ]
    }
   ],
   "source": [
    "# Continue training the network\n",
    "print(\"Continue training the network...\")\n",
    "initial_training_time = train_model(model, train_loader, num_epochs)\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(4096, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 100),\n",
    ")\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the initial network...\n",
      "Epoch [1/10], Loss: 3.9360, Accuracy: 9.58%\n",
      "Epoch [2/10], Loss: 3.2361, Accuracy: 19.91%\n",
      "Epoch [3/10], Loss: 2.8938, Accuracy: 26.38%\n",
      "Epoch [4/10], Loss: 2.7147, Accuracy: 30.13%\n",
      "Epoch [5/10], Loss: 2.5752, Accuracy: 32.80%\n",
      "Epoch [6/10], Loss: 2.4843, Accuracy: 34.69%\n",
      "Epoch [7/10], Loss: 2.4160, Accuracy: 36.10%\n",
      "Epoch [8/10], Loss: 2.3641, Accuracy: 37.27%\n",
      "Epoch [9/10], Loss: 2.3018, Accuracy: 38.64%\n",
      "Epoch [10/10], Loss: 2.2578, Accuracy: 39.25%\n",
      "Training completed in: 300.83 seconds\n",
      "Test Accuracy: 35.20%\n"
     ]
    }
   ],
   "source": [
    "# Training the initial network\n",
    "print(\"Training the initial network...\")\n",
    "initial_training_time = train_model(model, train_loader, num_epochs)\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: Evolving Without Restarting\n",
    "\n",
    "The ability to insert new layers, neurons, or filters into a trained neural network without losing accuracy represents a significant breakthrough in neural network training. By leveraging identity matrices and zero-weighted connections, we can preserve the model’s learned knowledge, while the ActiSwitch layer ensures smooth transitions between activation functions. This innovation opens up new possibilities for evolving neural network architectures and allows researchers to push the boundaries of model accuracy without retraining from scratch.\n",
    "\n",
    "#### In a field where every hour of training counts, this method enables you to \"Keep the Momentum\" and continue improving your models without unnecessary delays or wasted resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
